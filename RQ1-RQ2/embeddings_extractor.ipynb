{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similaritysimilar_fixes\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tokens_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d952ee4",
   "metadata": {},
   "source": [
    "Similarity Score Calculation for Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_issue(text):\n",
    "    response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-short\",\n",
    "                input=text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def embed_issues(df):\n",
    "    checkpoint_files = glob.glob(\"issues_embeddings_checkpoint_*.pkl\")\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        print(f\"Loading from checkpoint: {latest_checkpoint}\")\n",
    "        df = pd.read_pickle(latest_checkpoint)\n",
    "        start_index = df[df[\"embedding\"].isnull()].index.min()\n",
    "    else:\n",
    "        df[\"embedding\"] = pd.Series([None]*len(df), dtype=object)\n",
    "        start_index = 0\n",
    "\n",
    "    for i in range(start_index, len(df)):\n",
    "        text = df.at[i, \"complete_text\"]\n",
    "        wait_time = 5\n",
    "        try:\n",
    "            response = embed_issue(text)\n",
    "            df.at[i, \"embedding\"] = response.data[0].embedding\n",
    "            if i % 100 == 0:\n",
    "                df.to_pickle(f\"issues_embeddings_checkpoint_{i}.pkl\")\n",
    "                print(f\"Processed {i} issues, checkpoint saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i}: {e}. Retrying in {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time = min(wait_time * 2, 60) \n",
    "    return df\n",
    "\n",
    "def analyze_neighbors(sim_matrix, k, df, tfidf_matrix, embedding_matrix):\n",
    "    neighbors = []\n",
    "    for idx, row in df.iterrows():\n",
    "        sim_scores = sim_matrix[idx]\n",
    "        neighbor_indices = sim_scores.argsort()[-(k+1):-1][::-1]\n",
    "        neighbor_info = [(df.at[n_idx, 'number'], sim_scores[n_idx].round(4), tfidf_matrix[idx][n_idx].round(4), embedding_matrix[idx][n_idx].round(4)) for n_idx in neighbor_indices]\n",
    "        neighbors.append({\n",
    "            'issue_number': row['number'],\n",
    "            'neighbors': neighbor_info\n",
    "        })\n",
    "    return neighbors\n",
    "\n",
    "def normalize_neighbors(neighbors_list, threshold=0.7):\n",
    "    pairs = []\n",
    "    for item in neighbors_list:\n",
    "        issue = item['issue_number']\n",
    "        for neighbor, score, tf_idf, embed in item['neighbors']:\n",
    "            if score >= threshold:\n",
    "                pairs.append((issue, neighbor.item(), score.item(), tf_idf.item(), embed.item()))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def erase_pair_duplicates(pairs):\n",
    "    unique_pairs = set()\n",
    "    for issue1, issue2, score, tfidf, embed in pairs:\n",
    "        if issue1 < issue2:\n",
    "            unique_pairs.add((issue1, issue2, score, tfidf, embed))\n",
    "        else:\n",
    "            unique_pairs.add((issue2, issue1, score, tfidf, embed))\n",
    "    return list(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing tokens from 8788 to 8190\n",
      "Reducing tokens from 15446 to 8190\n",
      "Reducing tokens from 16490 to 8190\n",
      "Reducing tokens from 9172 to 8190\n",
      "Reducing tokens from 15766 to 8190\n",
      "Reducing tokens from 14999 to 8190\n",
      "Reducing tokens from 12524 to 8190\n",
      "Reducing tokens from 8521 to 8190\n",
      "Reducing tokens from 8481 to 8190\n",
      "Reducing tokens from 8218 to 8190\n",
      "Reducing tokens from 11361 to 8190\n",
      "Reducing tokens from 9732 to 8190\n",
      "Reducing tokens from 12119 to 8190\n",
      "Reducing tokens from 11304 to 8190\n",
      "Reducing tokens from 9011 to 8190\n",
      "Reducing tokens from 8882 to 8190\n",
      "Reducing tokens from 10599 to 8190\n",
      "Reducing tokens from 10723 to 8190\n",
      "Reducing tokens from 19644 to 8190\n",
      "Reducing tokens from 8912 to 8190\n",
      "Reducing tokens from 8690 to 8190\n",
      "Reducing tokens from 10832 to 8190\n",
      "Reducing tokens from 8238 to 8190\n",
      "Reducing tokens from 18864 to 8190\n",
      "Reducing tokens from 9857 to 8190\n",
      "Reducing tokens from 11522 to 8190\n",
      "Reducing tokens from 15850 to 8190\n",
      "Reducing tokens from 11997 to 8190\n",
      "Reducing tokens from 12574 to 8190\n",
      "Reducing tokens from 10037 to 8190\n",
      "Reducing tokens from 18726 to 8190\n",
      "Reducing tokens from 8336 to 8190\n",
      "Reducing tokens from 8194 to 8190\n",
      "Reducing tokens from 8193 to 8190\n",
      "Reducing tokens from 18404 to 8190\n",
      "Reducing tokens from 8334 to 8190\n",
      "Reducing tokens from 10334 to 8190\n",
      "Reducing tokens from 11906 to 8190\n",
      "Reducing tokens from 18506 to 8190\n",
      "Reducing tokens from 15426 to 8190\n",
      "Reducing tokens from 8354 to 8190\n",
      "Reducing tokens from 10861 to 8190\n",
      "Reducing tokens from 14147 to 8190\n",
      "Reducing tokens from 75478 to 8190\n",
      "Reducing tokens from 11067 to 8190\n",
      "Reducing tokens from 29662 to 8190\n",
      "Reducing tokens from 12208 to 8190\n",
      "Reducing tokens from 14091 to 8190\n",
      "Reducing tokens from 27923 to 8190\n",
      "Reducing tokens from 10960 to 8190\n",
      "Reducing tokens from 8211 to 8190\n",
      "Reducing tokens from 11669 to 8190\n",
      "Reducing tokens from 12721 to 8190\n",
      "Reducing tokens from 9908 to 8190\n",
      "Reducing tokens from 8218 to 8190\n",
      "Reducing tokens from 10170 to 8190\n",
      "Reducing tokens from 9788 to 8190\n",
      "Reducing tokens from 18361 to 8190\n",
      "Reducing tokens from 16505 to 8190\n",
      "Reducing tokens from 19611 to 8190\n",
      "Reducing tokens from 8225 to 8190\n",
      "Reducing tokens from 26734 to 8190\n",
      "Reducing tokens from 27106 to 8190\n",
      "Reducing tokens from 11215 to 8190\n",
      "Reducing tokens from 8192 to 8190\n",
      "Reducing tokens from 8193 to 8190\n",
      "Reducing tokens from 9864 to 8190\n",
      "Reducing tokens from 8216 to 8190\n",
      "Reducing tokens from 8610 to 8190\n",
      "Reducing tokens from 9876 to 8190\n",
      "Reducing tokens from 24581 to 8190\n",
      "Reducing tokens from 15478 to 8190\n",
      "Reducing tokens from 9918 to 8190\n",
      "Reducing tokens from 13116 to 8190\n",
      "Reducing tokens from 31243 to 8190\n",
      "Reducing tokens from 8740 to 8190\n",
      "Reducing tokens from 8192 to 8190\n",
      "Reducing tokens from 8192 to 8190\n",
      "Reducing tokens from 8321 to 8190\n",
      "Reducing tokens from 9123 to 8190\n",
      "Reducing tokens from 9050 to 8190\n",
      "Reducing tokens from 11693 to 8190\n",
      "Reducing tokens from 8197 to 8190\n",
      "Reducing tokens from 15451 to 8190\n",
      "Reducing tokens from 17054 to 8190\n",
      "Reducing tokens from 9182 to 8190\n",
      "Reducing tokens from 8464 to 8190\n",
      "Reducing tokens from 29433 to 8190\n",
      "Reducing tokens from 10223 to 8190\n",
      "Reducing tokens from 8478 to 8190\n",
      "Reducing tokens from 8225 to 8190\n",
      "Reducing tokens from 8315 to 8190\n",
      "Reducing tokens from 19874 to 8190\n",
      "Reducing tokens from 20046 to 8190\n",
      "Reducing tokens from 8271 to 8190\n",
      "Reducing tokens from 8336 to 8190\n",
      "Reducing tokens from 9833 to 8190\n"
     ]
    }
   ],
   "source": [
    "# We recommend doing this analysis project by project.\n",
    "repo = 'pytorch'\n",
    "\n",
    "# We include a copy of the processed issues (text cleaned for tf-idf analysis)\n",
    "# For reference on how we cleaned data, go to: text_cleaning.ipynb\n",
    "df_processed = pd.read_pickle(f\"processed_issues/{repo}_processed.pkl\")\n",
    "df_processed['description_tokens'] = df_processed['description_tokens'].apply(reduce_tokens)\n",
    "for idx, row in df_processed.iterrows():\n",
    "    text, comments = reduce_tokens_with_comments(row['description_tokens'], row['comments_tokens'])\n",
    "    df_processed.at[idx, 'complete_text'] = text + \" \" + comments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3acb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = 'ComfyUI'\n",
    "df_processed = pd.read_pickle(f\"processed_issues/{repo}_processed.pkl\")\n",
    "\n",
    "if os.path.exists(f\"issues_embeddings/{repo}_embeddings.pkl\"):\n",
    "    df_with_embeddings = pd.read_pickle(f\"issues_embeddings/{repo}_embeddings.pkl\")\n",
    "else:\n",
    "    df_with_embeddings = embed_issues(df_processed)\n",
    "    df_with_embeddings.to_pickle(f\"issues_embeddings/{repo}_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "eedeb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/widni/Documents/pyExplore/py-git/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Similarity Score: TF-IDF (using cosine similarity)\n",
    "issues = df_processed['complete_text'].tolist()\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, tokenizer=lambda x: x.split())\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(issues)\n",
    "tfidf_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "tfidf_sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "d46cea7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Similarity Score: Embeddings (using cosine similarity)\n",
    "embeddings = df_with_embeddings['embedding'].tolist()\n",
    "embeddings_matrix = np.array(embeddings)\n",
    "embeddings_sim_matrix = cosine_similarity(embeddings_matrix)\n",
    "embeddings_sim_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "d8456159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Similarity Score: S1 + S2 --> Take only the 3 nearest neighbors per issue\n",
    "combined_sim_matrix = tfidf_sim_matrix + embeddings_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_to_analyze = analyze_neighbors(combined_sim_matrix, 3, df_with_embeddings, tfidf_sim_matrix, embeddings_sim_matrix)\n",
    "pairs = erase_pair_duplicates(normalize_neighbors(pairs_to_analyze))\n",
    "df_pairs = pd.DataFrame(pairs, columns=['issue_1', 'issue_2', 'similarity_score', 'tf_idf_score', 'embedding_score'])\n",
    "df_pairs = df_pairs.sort_values(by='similarity_score', ascending=False)\n",
    "df_pairs.to_csv(f\"similar_issues/{repo}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e40912",
   "metadata": {},
   "source": [
    "Similarity Scores for Issues can be found in the data downloaded folder: RQ1/similar_issues/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898358e",
   "metadata": {},
   "source": [
    "Similarity Score Calculation for Pull Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd9235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>bodyText</th>\n",
       "      <th>mergedAt</th>\n",
       "      <th>owner</th>\n",
       "      <th>repo</th>\n",
       "      <th>diff</th>\n",
       "      <th>oid</th>\n",
       "      <th>message</th>\n",
       "      <th>committedDate</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>diff_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6357.0</td>\n",
       "      <td>Document get_attr and get_model_object</td>\n",
       "      <td>https://github.com/comfyanonymous/ComfyUI/pull...</td>\n",
       "      <td>Resolves #6303\\nThis PR adds documentation on ...</td>\n",
       "      <td>2025-01-07T01:12:22Z</td>\n",
       "      <td>comfyanonymous</td>\n",
       "      <td>ComfyUI</td>\n",
       "      <td>diff --git a/comfy/model_patcher.py b/comfy/mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6335.0</td>\n",
       "      <td>Update web content to release v1.6.16</td>\n",
       "      <td>https://github.com/comfyanonymous/ComfyUI/pull...</td>\n",
       "      <td>Resolves #6333\\nCherry-pick following PRs to 1...</td>\n",
       "      <td>2025-01-03T18:56:47Z</td>\n",
       "      <td>comfyanonymous</td>\n",
       "      <td>ComfyUI</td>\n",
       "      <td>diff --git a/web/assets/BaseViewTemplate-Bklhd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6300.0</td>\n",
       "      <td>Convert `latents_ubyte` to 8-bit unsigned int ...</td>\n",
       "      <td>https://github.com/comfyanonymous/ComfyUI/pull...</td>\n",
       "      <td>Hello, not sure if this is the best solution, ...</td>\n",
       "      <td>2025-01-28T13:22:54Z</td>\n",
       "      <td>comfyanonymous</td>\n",
       "      <td>ComfyUI</td>\n",
       "      <td>diff --git a/latent_preview.py b/latent_previe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7244.0</td>\n",
       "      <td>Update frontend to 1.12.14</td>\n",
       "      <td>https://github.com/comfyanonymous/ComfyUI/pull...</td>\n",
       "      <td>Cherry pick Comfy-Org/ComfyUI_frontend#3065\\nd...</td>\n",
       "      <td>2025-03-15T05:38:10Z</td>\n",
       "      <td>comfyanonymous</td>\n",
       "      <td>ComfyUI</td>\n",
       "      <td>diff --git a/requirements.txt b/requirements.t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7143.0</td>\n",
       "      <td>Fix LoadImageOutput node</td>\n",
       "      <td>https://github.com/comfyanonymous/ComfyUI/pull...</td>\n",
       "      <td>The frontend now annotates uploaded image file...</td>\n",
       "      <td>2025-03-11T08:30:25Z</td>\n",
       "      <td>comfyanonymous</td>\n",
       "      <td>ComfyUI</td>\n",
       "      <td>diff --git a/nodes.py b/nodes.py\\nindex bbf499...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18231</th>\n",
       "      <td>48216.0</td>\n",
       "      <td>[v1.7.1] [complex] torch.sqrt: fix edge values...</td>\n",
       "      <td>https://github.com/pytorch/pytorch/pull/48216</td>\n",
       "      <td>Summary:\\nFixes #47358\\nReplace the optimized ...</td>\n",
       "      <td>2020-11-19T17:25:00Z</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>diff --git a/aten/src/ATen/cpu/vec256/vec256_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18232</th>\n",
       "      <td>48215.0</td>\n",
       "      <td>[v1.7.1] Make sure valid ParameterList/Dict do...</td>\n",
       "      <td>https://github.com/pytorch/pytorch/pull/48215</td>\n",
       "      <td>Summary:\\nFixes #46983\\nPull Request resolved:...</td>\n",
       "      <td>2020-11-19T17:24:08Z</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>diff --git a/test/test_nn.py b/test/test_nn.py...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18233</th>\n",
       "      <td>48936.0</td>\n",
       "      <td>Disable autocast cache for tensor views as fix...</td>\n",
       "      <td>https://github.com/pytorch/pytorch/pull/48936</td>\n",
       "      <td>Summary:\\nFixes #48049\\nRoot cause of the issu...</td>\n",
       "      <td>2020-12-07T19:28:38Z</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>diff --git a/aten/src/ATen/autocast_mode.cpp b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18234</th>\n",
       "      <td>48768.0</td>\n",
       "      <td>[1.7.1] torch: Stop using _nt_quote_args from ...</td>\n",
       "      <td>https://github.com/pytorch/pytorch/pull/48768</td>\n",
       "      <td>Summary:\\nThey removed the specific function i...</td>\n",
       "      <td>2020-12-03T04:41:56Z</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>diff --git a/torch/utils/cpp_extension.py b/to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18235</th>\n",
       "      <td>48744.0</td>\n",
       "      <td>[Release/1.7] Fix index parsing on Python-3.9</td>\n",
       "      <td>https://github.com/pytorch/pytorch/pull/48744</td>\n",
       "      <td>Summary:\\nIn 3.9, ast.Index and ast.ExtSlice a...</td>\n",
       "      <td>2020-12-02T21:20:48Z</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>pytorch</td>\n",
       "      <td>diff --git a/torch/jit/frontend.py b/torch/jit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18236 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        number                                              title  \\\n",
       "0       6357.0             Document get_attr and get_model_object   \n",
       "1       6335.0              Update web content to release v1.6.16   \n",
       "2       6300.0  Convert `latents_ubyte` to 8-bit unsigned int ...   \n",
       "3       7244.0                         Update frontend to 1.12.14   \n",
       "4       7143.0                           Fix LoadImageOutput node   \n",
       "...        ...                                                ...   \n",
       "18231  48216.0  [v1.7.1] [complex] torch.sqrt: fix edge values...   \n",
       "18232  48215.0  [v1.7.1] Make sure valid ParameterList/Dict do...   \n",
       "18233  48936.0  Disable autocast cache for tensor views as fix...   \n",
       "18234  48768.0  [1.7.1] torch: Stop using _nt_quote_args from ...   \n",
       "18235  48744.0      [Release/1.7] Fix index parsing on Python-3.9   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://github.com/comfyanonymous/ComfyUI/pull...   \n",
       "1      https://github.com/comfyanonymous/ComfyUI/pull...   \n",
       "2      https://github.com/comfyanonymous/ComfyUI/pull...   \n",
       "3      https://github.com/comfyanonymous/ComfyUI/pull...   \n",
       "4      https://github.com/comfyanonymous/ComfyUI/pull...   \n",
       "...                                                  ...   \n",
       "18231      https://github.com/pytorch/pytorch/pull/48216   \n",
       "18232      https://github.com/pytorch/pytorch/pull/48215   \n",
       "18233      https://github.com/pytorch/pytorch/pull/48936   \n",
       "18234      https://github.com/pytorch/pytorch/pull/48768   \n",
       "18235      https://github.com/pytorch/pytorch/pull/48744   \n",
       "\n",
       "                                                bodyText  \\\n",
       "0      Resolves #6303\\nThis PR adds documentation on ...   \n",
       "1      Resolves #6333\\nCherry-pick following PRs to 1...   \n",
       "2      Hello, not sure if this is the best solution, ...   \n",
       "3      Cherry pick Comfy-Org/ComfyUI_frontend#3065\\nd...   \n",
       "4      The frontend now annotates uploaded image file...   \n",
       "...                                                  ...   \n",
       "18231  Summary:\\nFixes #47358\\nReplace the optimized ...   \n",
       "18232  Summary:\\nFixes #46983\\nPull Request resolved:...   \n",
       "18233  Summary:\\nFixes #48049\\nRoot cause of the issu...   \n",
       "18234  Summary:\\nThey removed the specific function i...   \n",
       "18235  Summary:\\nIn 3.9, ast.Index and ast.ExtSlice a...   \n",
       "\n",
       "                   mergedAt           owner     repo  \\\n",
       "0      2025-01-07T01:12:22Z  comfyanonymous  ComfyUI   \n",
       "1      2025-01-03T18:56:47Z  comfyanonymous  ComfyUI   \n",
       "2      2025-01-28T13:22:54Z  comfyanonymous  ComfyUI   \n",
       "3      2025-03-15T05:38:10Z  comfyanonymous  ComfyUI   \n",
       "4      2025-03-11T08:30:25Z  comfyanonymous  ComfyUI   \n",
       "...                     ...             ...      ...   \n",
       "18231  2020-11-19T17:25:00Z         pytorch  pytorch   \n",
       "18232  2020-11-19T17:24:08Z         pytorch  pytorch   \n",
       "18233  2020-12-07T19:28:38Z         pytorch  pytorch   \n",
       "18234  2020-12-03T04:41:56Z         pytorch  pytorch   \n",
       "18235  2020-12-02T21:20:48Z         pytorch  pytorch   \n",
       "\n",
       "                                                    diff  oid message  \\\n",
       "0      diff --git a/comfy/model_patcher.py b/comfy/mo...  NaN     NaN   \n",
       "1      diff --git a/web/assets/BaseViewTemplate-Bklhd...  NaN     NaN   \n",
       "2      diff --git a/latent_preview.py b/latent_previe...  NaN     NaN   \n",
       "3      diff --git a/requirements.txt b/requirements.t...  NaN     NaN   \n",
       "4      diff --git a/nodes.py b/nodes.py\\nindex bbf499...  NaN     NaN   \n",
       "...                                                  ...  ...     ...   \n",
       "18231  diff --git a/aten/src/ATen/cpu/vec256/vec256_c...  NaN     NaN   \n",
       "18232  diff --git a/test/test_nn.py b/test/test_nn.py...  NaN     NaN   \n",
       "18233  diff --git a/aten/src/ATen/autocast_mode.cpp b...  NaN     NaN   \n",
       "18234  diff --git a/torch/utils/cpp_extension.py b/to...  NaN     NaN   \n",
       "18235  diff --git a/torch/jit/frontend.py b/torch/jit...  NaN     NaN   \n",
       "\n",
       "      committedDate  diff_len diff_embedding  \n",
       "0               NaN       NaN            NaN  \n",
       "1               NaN       NaN            NaN  \n",
       "2               NaN       NaN            NaN  \n",
       "3               NaN       NaN            NaN  \n",
       "4               NaN       NaN            NaN  \n",
       "...             ...       ...            ...  \n",
       "18231           NaN       NaN            NaN  \n",
       "18232           NaN       NaN            NaN  \n",
       "18233           NaN       NaN            NaN  \n",
       "18234           NaN       NaN            NaN  \n",
       "18235           NaN       NaN            NaN  \n",
       "\n",
       "[18236 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs_pkl = glob.glob(os.path.join('./prs_per_project/', '*.pkl'))\n",
    "prs_df = pd.concat([pd.read_pickle(file) for file in prs_pkl], ignore_index=True)\n",
    "# We control the length of the diffs for the model selected. \n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "prs_df['diff_len'] = prs_df['diff'].apply(lambda x: len(encoding.encode(x, disallowed_special=())))\n",
    "# We discard longer diffs\n",
    "df_with_embeddings = prs_df[prs_df['diff_len'] <= 8191]\n",
    "# Extract embeddings, we recommend to extract by parts\n",
    "for idx, row in df_ready.iterrows():\n",
    "    df_with_embeddings.at[idx, 'diff_embedding'] = get_embedding(row['diff'])\n",
    "\n",
    "# All prs with embeddings can be found in RQ2/embeddings_fixes/\n",
    "\n",
    "# Calculate nearest neighbor\n",
    "\n",
    "X = np.vstack(df_with_embeddings[\"diff_embedding\"].values)\n",
    "knn = NearestNeighbors(\n",
    "    n_neighbors=2,  \n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "knn.fit(X)\n",
    "distances, indices = knn.kneighbors(X)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, (dists, idxs) in enumerate(zip(distances, indices)):\n",
    "    dist = dists[1]\n",
    "    j = idxs[1]\n",
    "    rows.append({\n",
    "        \"source_url\": df_with_embeddings.iloc[i][\"url\"],\n",
    "        \"nearest_url\": df_with_embeddings.iloc[j][\"url\"],\n",
    "        \"cosine_similarity\": 1 - dist\n",
    "    })\n",
    "\n",
    "nn_df = pd.DataFrame(rows)\n",
    "\n",
    "nn_df[[\"source_url\", \"nearest_url\"]] = nn_df.apply(\n",
    "    lambda r: sorted([r[\"source_url\"], r[\"nearest_url\"]]),\n",
    "    axis=1,\n",
    "    result_type=\"expand\"\n",
    ")\n",
    "\n",
    "nn_df = (\n",
    "    nn_df\n",
    "    .groupby([\"source_url\", \"nearest_url\"], as_index=False)\n",
    "    [\"cosine_similarity\"]\n",
    "    .max()\n",
    ")\n",
    "\n",
    "nn_df = nn_df.sort_values(\n",
    "    by=\"cosine_similarity\",\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "repo = file.split('/')[2].split('_')[0]\n",
    "nn_df.to_csv(f'./similar_fixes/{repo}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195f234",
   "metadata": {},
   "source": [
    "Similarity Scores for Pull Requests can be found in the data downloaded folder: RQ2/similar_fixes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
